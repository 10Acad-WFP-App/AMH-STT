{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import numpy as np\n",
    "import soundfile\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "\n",
    "from scripts.audio_explorer import AudioExplorer\n",
    "from scripts.dataset_preparer import make_audio_gen\n",
    "from scripts.Utility import char_map\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "\n",
    "import _pickle as pickle\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 19:48:16.039227: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-07 19:48:16.039258: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def model_1(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='models/model_1.png')\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "        outputs=loss_out)\n",
    "    return model\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def train(audio_gen,\n",
    "          input_to_softmax, \n",
    "          model_name,\n",
    "          minibatch_size=20,\n",
    "          optimizer=SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          epochs=20,\n",
    "          verbose=1):    \n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    \n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "\n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath='models/'+model_name+'.h5', verbose=0)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
    "        callbacks=[checkpointer], verbose=verbose, use_multiprocessing=True)\n",
    "\n",
    "    # save model loss\n",
    "    with open('models/'+model_name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(hist.history, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "TRAIN_CORPUS = \"../train.json\"\n",
    "VALID_CORPUS = \"../valid.json\"\n",
    "\n",
    "MFCC_DIM = 13\n",
    "SPECTOGRAM = False\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = \"RNN_model\"\n",
    "\n",
    "################ Reminder MINI_BATCH_SIZE=250 \n",
    "MINI_BATCH_SIZE = 250\n",
    "\n",
    "SORT_BY_DURATION=False\n",
    "MAX_DURATION = 10.0\n",
    "\n",
    "audio_gen = make_audio_gen(TRAIN_CORPUS, VALID_CORPUS, spectrogram=False, mfcc_dim=MFCC_DIM,\n",
    "                           minibatch_size=MINI_BATCH_SIZE, sort_by_duration=SORT_BY_DURATION,\n",
    "                           max_duration=MAX_DURATION)\n",
    "# add the training data to the generator\n",
    "audio_gen.load_train_data()\n",
    "audio_gen.load_validation_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = model_1(input_dim=13,\n",
    "                units=5,\n",
    "                activation='relu',\n",
    "                output_dim=len(char_map)+1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None, 13)]        0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 5)           300       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 5)           20        \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 290)         1740      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 290)         0         \n",
      "=================================================================\n",
      "Total params: 2,060\n",
      "Trainable params: 2,050\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 19:48:30.871896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-07 19:48:30.871955: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-07 19:48:30.871995: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dibora): /proc/driver/nvidia/version does not exist\n",
      "2021-08-07 19:48:30.872503: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train(audio_gen, input_to_softmax=model, model_name=MODEL_NAME, epochs=1, minibatch_size=MINI_BATCH_SIZE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2021-08-07 19:49:03.187541: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37921000 exceeds 10% of free system memory.\n",
      "2021-08-07 19:49:03.835606: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-07 19:49:03.885521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1800000000 Hz\n",
      "2021-08-07 19:49:29.622039: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37921000 exceeds 10% of free system memory.\n",
      "2021-08-07 19:49:30.242586: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37921000 exceeds 10% of free system memory.\n",
      "2021-08-07 19:49:34.602935: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 845930000 exceeds 10% of free system memory.\n",
      "2021-08-07 19:49:39.361274: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 845930000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/2 [==============>...............] - ETA: 3:51 - loss: 14309.0508"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 19:55:17.038466: W tensorflow/core/framework/op_kernel.cc:1755] Unknown: AttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 807, in next_sample\n",
      "    return next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 157, in next_train\n",
      "    self.shuffle_data_by_partition('train')\n",
      "  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 126, in shuffle_data_by_partition\n",
      "    self.train_audio_paths, self.train_durations, self.train_texts)\n",
      "AttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 961, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/data_adapter.py\", line 821, in wrapped_generator\n",
      "    for data in generator_fn():\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 894, in get\n",
      "    raise e\n",
      "\n",
      "  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 870, in get\n",
      "    inputs = self.queue.get(block=True).get()\n",
      "\n",
      "  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "\n",
      "AttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnknownError",
     "evalue": " AttributeError: 'AudioGenerator' object has no attribute 'train_durations'\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 807, in next_sample\n    return next(_SHARED_SEQUENCES[uid])\n  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 157, in next_train\n    self.shuffle_data_by_partition('train')\n  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 126, in shuffle_data_by_partition\n    self.train_audio_paths, self.train_durations, self.train_texts)\nAttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 961, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/data_adapter.py\", line 821, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 894, in get\n    raise e\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 870, in get\n    inputs = self.queue.get(block=True).get()\n\n  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n\nAttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3027]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112999/416686350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMINI_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_112999/2177974053.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(audio_gen, input_to_softmax, model_name, minibatch_size, optimizer, epochs, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m     hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n\u001b[1;32m     30\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         callbacks=[checkpointer], verbose=verbose, use_multiprocessing=True)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# save model loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  AttributeError: 'AudioGenerator' object has no attribute 'train_durations'\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 807, in next_sample\n    return next(_SHARED_SEQUENCES[uid])\n  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 157, in next_train\n    self.shuffle_data_by_partition('train')\n  File \"/home/dibora/AMH-STT/scripts/dataset_preparer.py\", line 126, in shuffle_data_by_partition\n    self.train_audio_paths, self.train_durations, self.train_texts)\nAttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n\"\"\"\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 961, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/engine/data_adapter.py\", line 821, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 894, in get\n    raise e\n\n  File \"/home/dibora/AMH-STT/STT/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 870, in get\n    inputs = self.queue.get(block=True).get()\n\n  File \"/home/dibora/miniconda3/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n\nAttributeError: 'AudioGenerator' object has no attribute 'train_durations'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3027]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('STT': venv)"
  },
  "interpreter": {
   "hash": "0e39dd039c79894c5c8eaa0308b24d5ba0115abe9baefb5e95928432ee3a3fd5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}