{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ba901e-04fd-4370-8349-5922a37763aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "import glob\n",
    "import numpy as np\n",
    "from six.moves import xrange as range\n",
    "import json\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import (Input, Lambda)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint  \n",
    "import librosa \n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "import gc\n",
    "\n",
    "# import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c17201-9ec8-4e7d-a59c-07d80a01a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "SPACE_TOKEN = '<space>'\n",
    "SPACE_INDEX = 0\n",
    "FIRST_INDEX = 1\n",
    "FEAT_MASK_VALUE = 1e+10\n",
    "\n",
    "# Some configs\n",
    "# filters=200\n",
    "# kernel_size=11\n",
    "# conv_stride=2\n",
    "# conv_border_mode='valid'\n",
    "units=200\n",
    "num_features = 13\n",
    "num_units = 100\n",
    "num_classes = 222 + 1 # 285(including space) + blamk label = 286\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 100\n",
    "num_layers = 1\n",
    "batch_size = 16\n",
    "initial_learning_rate = 0.0005\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe58f0c6-de98-40b6-ab43-3dd420eefc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "file_path = glob.glob('/home/dibora_gebreyohannes/AMH-STT/data/train/wav/*.wav')\n",
    "# file_path = file_path[28:32]\n",
    "audio_list = []\n",
    "fs_list = []\n",
    "dur_list = []\n",
    "dropped_file_path = []\n",
    "\n",
    "for file_name in file_path:\n",
    "    audio,fs = librosa.load(file_name,sr=16000)\n",
    "    dur = librosa.get_duration(audio,sr=16000)\n",
    "    if dur > 2 and dur < 6:\n",
    "        dropped_file_path.append(file_name)\n",
    "        audio_list.append(audio)\n",
    "        dur_list.append(dur)\n",
    "        fs_list.append(fs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ef3f13-0679-4128-a785-46f813715e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdb52b7-cf76-4083-bbd3-288ad8cc3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset composed of data with variable lengths\n",
    "inputs_list = []\n",
    "for index in range(len(audio_list)):\n",
    "    input_val = mfcc(audio_list[index], samplerate=fs_list[index])\n",
    "    input_val = (input_val - np.mean(input_val)) / np.std(input_val)\n",
    "    inputs_list.append(input_val)\n",
    "\n",
    "# Transform in 3D Array\n",
    "train_inputs = tf.ragged.constant([i for i in inputs_list], dtype=np.float32)\n",
    "train_seq_len = tf.cast(train_inputs.row_lengths(), tf.int32)\n",
    "train_inputs = train_inputs.to_tensor(default_value=FEAT_MASK_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c773b459-59ae-44c1-8136-a366baa4bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.json', 'r', encoding='UTF-8') as label_file:\n",
    "    labels = json.load(label_file)\n",
    "with open('language_model.json', 'r', encoding='UTF-8') as language_file:\n",
    "    alphabets = json.load(language_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6866b469-9e4b-425e-817f-fc5b2a3f25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Targets\n",
    "original_list = []\n",
    "targets_list = []\n",
    "\n",
    "for path in dropped_file_path:\n",
    "    file_name = path[:-4].split('wav')[1][1:]\n",
    "    # Read Label\n",
    "    label = labels[file_name]\n",
    "    original = \" \".join(label.strip().split(' '))\n",
    "    original_list.append(original)\n",
    "#     print(original)\n",
    "    target = original.replace(' ', '  ')\n",
    "    # print('step-1. ',target)\n",
    "    target = target.split(' ')\n",
    "    # print('step-2. ', target)\n",
    "    # Adding blank label\n",
    "    target = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in target])\n",
    "    # print('step-3. ', target)\n",
    "    # Transform char into index\n",
    "    target = np.asarray([alphabets['char_to_num'][x] for x in target])\n",
    "    # print('step-4. ', target)\n",
    "    targets_list.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b00706-84e4-4b68-82fe-863431fa5dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a538fce2-5e7b-4a68-a1e8-c94bf3c75ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sparse representation to feed the placeholder\n",
    "train_targets = tf.ragged.constant([i for i in targets_list], dtype=np.int32)\n",
    "train_targets_len = tf.cast(train_targets.row_lengths(), tf.int32)\n",
    "train_targets = train_targets.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd1d6c7-6281-4129-a4f5-6056ff0919d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs, val_targets, val_seq_len, val_targets_len = train_inputs, train_targets, train_seq_len, train_targets_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26420e6-239a-40d4-9169-ac3f1ec73681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLossLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        labels = inputs[0]\n",
    "        logits = inputs[1]\n",
    "        label_len = inputs[2]\n",
    "        logit_len = inputs[3]\n",
    "\n",
    "        logits_trans = tf.transpose(logits, (1,0,2))\n",
    "        label_len = tf.reshape(label_len, (-1,))\n",
    "        logit_len = tf.reshape(logit_len, (-1,))\n",
    "        loss = tf.reduce_mean(tf.nn.ctc_loss(labels, logits_trans, label_len, logit_len, blank_index=-1))\n",
    "        # define loss here instead of in compile\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # Decode\n",
    "        decoded, _ = tf.nn.ctc_greedy_decoder(logits_trans, logit_len)\n",
    "\n",
    "        # Inaccuracy: label error rate\n",
    "        ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),labels))\n",
    "        self.add_metric(ler, name='ler', aggregation='mean')\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2068651d-1947-48fb-bedb-a671095f20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Training Cells\n",
    "num_units = 50\n",
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "    cell = tf.keras.layers.GRUCell(num_units)\n",
    "    cells.append(cell)\n",
    "\n",
    "stack = tf.keras.layers.StackedRNNCells(cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08477705-ad36-44ea-b87e-a136207a3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definning Input Parameters\n",
    "input_feature = tf.keras.layers.Input((None, num_features), name='input_feature')\n",
    "input_label = tf.keras.layers.Input((None,), dtype=tf.int32, sparse=True, name='input_label')\n",
    "input_feature_len = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_feature_len')\n",
    "input_label_len =tf.keras.layers.Input((1,), dtype=tf.int32, name='input_label_len')\n",
    "\n",
    "input_masking = tf.keras.layers.Masking(FEAT_MASK_VALUE)(input_feature)\n",
    "x = tf.keras.layers.LSTM(100,return_sequences=True)(input_masking)\n",
    "x_1 = tf.keras.layers.BatchNormalization()(x)\n",
    "x_2 = tf.keras.layers.LSTM(100,return_sequences=True)(x_1)\n",
    "# x_3= tf.keras.layers.BatchNormalization()(x_2)\n",
    "# x = tf.keras.layers.LSTM(50,return_sequences=True)(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# layer_rnn = tf.keras.layers.LSTM(10, return_sequences=True)(layer_bn)\n",
    "# x = tf.keras.layers.Dropout(0.2, seed=42)(x)\n",
    "layer_output = tf.keras.layers.TimeDistributed(Dense(num_classes, kernel_initializer=tf.keras.initializers.TruncatedNormal(0.0,0.1), bias_initializer='zeros', name='logit'))(x_2)\n",
    "\n",
    "layer_loss = CTCLossLayer()([input_label, layer_output, input_label_len, input_feature_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebad848-3b83-4a0a-acc9-cc02c2659d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_feature (InputLayer)      [(None, None, 13)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 13)     0           input_feature[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 100)    45600       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 100)    400         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 100)    80400       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 223)    22523       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_label_len (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_feature_len (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss_layer (CTCLossLayer)   (None, None, 223)    0           input_label[0][0]                \n",
      "                                                                 time_distributed[0][0]           \n",
      "                                                                 input_label_len[0][0]            \n",
      "                                                                 input_feature_len[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 148,923\n",
      "Trainable params: 148,723\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create models for training and prediction\n",
    "model_train = tf.keras.models.Model(inputs=[input_feature, input_label, input_feature_len, input_label_len],\n",
    "            outputs=layer_loss)\n",
    "print(model_train.summary())\n",
    "model_predict = tf.keras.models.Model(inputs=input_feature, outputs=layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a3acd2-a149-4e10-8305-80a2d67ef2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "46/46 [==============================] - 12s 103ms/step - loss: 660.2971 - ler: 0.9289 - val_loss: 184.4509 - val_ler: 0.8990\n",
      "\n",
      "Epoch 00001: ler improved from inf to 0.92889, saving model to ../models/RNN.h5\n",
      "Epoch 2/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 138.7616 - ler: 0.8562 - val_loss: 144.6443 - val_ler: 0.8841\n",
      "\n",
      "Epoch 00002: ler improved from 0.92889 to 0.85615, saving model to ../models/RNN.h5\n",
      "Epoch 3/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 133.7938 - ler: 0.8626 - val_loss: 134.4492 - val_ler: 0.8675\n",
      "\n",
      "Epoch 00003: ler did not improve from 0.85615\n",
      "Epoch 4/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 128.5480 - ler: 0.8652 - val_loss: 130.7117 - val_ler: 0.8889\n",
      "\n",
      "Epoch 00004: ler did not improve from 0.85615\n",
      "Epoch 5/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 128.6340 - ler: 0.8661 - val_loss: 128.4174 - val_ler: 0.8642\n",
      "\n",
      "Epoch 00005: ler did not improve from 0.85615\n",
      "Epoch 6/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 127.9777 - ler: 0.8570 - val_loss: 127.4706 - val_ler: 0.8764\n",
      "\n",
      "Epoch 00006: ler did not improve from 0.85615\n",
      "Epoch 7/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 124.5137 - ler: 0.8594 - val_loss: 127.0539 - val_ler: 0.8476\n",
      "\n",
      "Epoch 00007: ler did not improve from 0.85615\n",
      "Epoch 8/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 125.0142 - ler: 0.8604 - val_loss: 126.1954 - val_ler: 0.8594\n",
      "\n",
      "Epoch 00008: ler did not improve from 0.85615\n",
      "Epoch 9/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 126.6279 - ler: 0.8478 - val_loss: 126.1520 - val_ler: 0.8527\n",
      "\n",
      "Epoch 00009: ler improved from 0.85615 to 0.84779, saving model to ../models/RNN.h5\n",
      "Epoch 10/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 125.3109 - ler: 0.8417 - val_loss: 125.6797 - val_ler: 0.8597\n",
      "\n",
      "Epoch 00010: ler improved from 0.84779 to 0.84167, saving model to ../models/RNN.h5\n",
      "Epoch 11/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 126.5406 - ler: 0.8443 - val_loss: 124.7606 - val_ler: 0.8572\n",
      "\n",
      "Epoch 00011: ler did not improve from 0.84167\n",
      "Epoch 12/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 125.9414 - ler: 0.8512 - val_loss: 124.4425 - val_ler: 0.8460\n",
      "\n",
      "Epoch 00012: ler did not improve from 0.84167\n",
      "Epoch 13/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 121.4927 - ler: 0.8425 - val_loss: 125.0311 - val_ler: 0.8428\n",
      "\n",
      "Epoch 00013: ler did not improve from 0.84167\n",
      "Epoch 14/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 126.6750 - ler: 0.8405 - val_loss: 123.6404 - val_ler: 0.8279\n",
      "\n",
      "Epoch 00014: ler improved from 0.84167 to 0.84052, saving model to ../models/RNN.h5\n",
      "Epoch 15/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 125.8416 - ler: 0.8308 - val_loss: 123.7762 - val_ler: 0.8517\n",
      "\n",
      "Epoch 00015: ler improved from 0.84052 to 0.83083, saving model to ../models/RNN.h5\n",
      "Epoch 16/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 124.4859 - ler: 0.8381 - val_loss: 123.0076 - val_ler: 0.8391\n",
      "\n",
      "Epoch 00016: ler did not improve from 0.83083\n",
      "Epoch 17/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 126.1275 - ler: 0.8338 - val_loss: 122.7062 - val_ler: 0.8344\n",
      "\n",
      "Epoch 00017: ler did not improve from 0.83083\n",
      "Epoch 18/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 122.0587 - ler: 0.8424 - val_loss: 123.3945 - val_ler: 0.8565\n",
      "\n",
      "Epoch 00018: ler did not improve from 0.83083\n",
      "Epoch 19/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 124.6848 - ler: 0.8452 - val_loss: 122.7457 - val_ler: 0.8289\n",
      "\n",
      "Epoch 00019: ler did not improve from 0.83083\n",
      "Epoch 20/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 124.3233 - ler: 0.8404 - val_loss: 122.4941 - val_ler: 0.8326\n",
      "\n",
      "Epoch 00020: ler did not improve from 0.83083\n",
      "Epoch 21/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 124.1404 - ler: 0.8325 - val_loss: 121.9272 - val_ler: 0.8356\n",
      "\n",
      "Epoch 00021: ler did not improve from 0.83083\n",
      "Epoch 22/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 118.6083 - ler: 0.8324 - val_loss: 121.4941 - val_ler: 0.8272\n",
      "\n",
      "Epoch 00022: ler did not improve from 0.83083\n",
      "Epoch 23/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 121.3050 - ler: 0.8293 - val_loss: 121.4816 - val_ler: 0.8449\n",
      "\n",
      "Epoch 00023: ler improved from 0.83083 to 0.82926, saving model to ../models/RNN.h5\n",
      "Epoch 24/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 119.5896 - ler: 0.8325 - val_loss: 120.3954 - val_ler: 0.8298\n",
      "\n",
      "Epoch 00024: ler did not improve from 0.82926\n",
      "Epoch 25/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 121.8657 - ler: 0.8236 - val_loss: 120.8997 - val_ler: 0.8357\n",
      "\n",
      "Epoch 00025: ler improved from 0.82926 to 0.82362, saving model to ../models/RNN.h5\n",
      "Epoch 26/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 120.3406 - ler: 0.8282 - val_loss: 120.4964 - val_ler: 0.8359\n",
      "\n",
      "Epoch 00026: ler did not improve from 0.82362\n",
      "Epoch 27/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 121.1406 - ler: 0.8195 - val_loss: 119.4159 - val_ler: 0.8249\n",
      "\n",
      "Epoch 00027: ler improved from 0.82362 to 0.81946, saving model to ../models/RNN.h5\n",
      "Epoch 28/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 119.3136 - ler: 0.8233 - val_loss: 119.6904 - val_ler: 0.8242\n",
      "\n",
      "Epoch 00028: ler did not improve from 0.81946\n",
      "Epoch 29/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 119.0197 - ler: 0.8229 - val_loss: 119.5458 - val_ler: 0.8132\n",
      "\n",
      "Epoch 00029: ler did not improve from 0.81946\n",
      "Epoch 30/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.3082 - ler: 0.8129 - val_loss: 119.1590 - val_ler: 0.8244\n",
      "\n",
      "Epoch 00030: ler improved from 0.81946 to 0.81292, saving model to ../models/RNN.h5\n",
      "Epoch 31/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 119.9720 - ler: 0.8143 - val_loss: 118.8197 - val_ler: 0.8137\n",
      "\n",
      "Epoch 00031: ler did not improve from 0.81292\n",
      "Epoch 32/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 116.4259 - ler: 0.8144 - val_loss: 117.9309 - val_ler: 0.8149\n",
      "\n",
      "Epoch 00032: ler did not improve from 0.81292\n",
      "Epoch 33/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.8045 - ler: 0.8095 - val_loss: 118.4185 - val_ler: 0.8075\n",
      "\n",
      "Epoch 00033: ler improved from 0.81292 to 0.80946, saving model to ../models/RNN.h5\n",
      "Epoch 34/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 119.4718 - ler: 0.8197 - val_loss: 117.9244 - val_ler: 0.8224\n",
      "\n",
      "Epoch 00034: ler did not improve from 0.80946\n",
      "Epoch 35/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 116.5622 - ler: 0.8170 - val_loss: 117.7715 - val_ler: 0.8294\n",
      "\n",
      "Epoch 00035: ler did not improve from 0.80946\n",
      "Epoch 36/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 117.8135 - ler: 0.8099 - val_loss: 117.3191 - val_ler: 0.8049\n",
      "\n",
      "Epoch 00036: ler did not improve from 0.80946\n",
      "Epoch 37/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.6319 - ler: 0.8070 - val_loss: 116.7709 - val_ler: 0.7982\n",
      "\n",
      "Epoch 00037: ler improved from 0.80946 to 0.80705, saving model to ../models/RNN.h5\n",
      "Epoch 38/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 116.4226 - ler: 0.7986 - val_loss: 118.6750 - val_ler: 0.8307\n",
      "\n",
      "Epoch 00038: ler improved from 0.80705 to 0.79858, saving model to ../models/RNN.h5\n",
      "Epoch 39/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.3527 - ler: 0.8039 - val_loss: 115.9460 - val_ler: 0.7939\n",
      "\n",
      "Epoch 00039: ler did not improve from 0.79858\n",
      "Epoch 40/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 117.7708 - ler: 0.8053 - val_loss: 118.0064 - val_ler: 0.7937\n",
      "\n",
      "Epoch 00040: ler did not improve from 0.79858\n",
      "Epoch 41/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.6168 - ler: 0.8017 - val_loss: 115.5423 - val_ler: 0.7987\n",
      "\n",
      "Epoch 00041: ler did not improve from 0.79858\n",
      "Epoch 42/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 116.8155 - ler: 0.8006 - val_loss: 115.4923 - val_ler: 0.7973\n",
      "\n",
      "Epoch 00042: ler did not improve from 0.79858\n",
      "Epoch 43/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 118.0465 - ler: 0.8028 - val_loss: 114.9081 - val_ler: 0.8007\n",
      "\n",
      "Epoch 00043: ler did not improve from 0.79858\n",
      "Epoch 44/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 115.2556 - ler: 0.7969 - val_loss: 114.5527 - val_ler: 0.8021\n",
      "\n",
      "Epoch 00044: ler improved from 0.79858 to 0.79688, saving model to ../models/RNN.h5\n",
      "Epoch 45/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 113.2042 - ler: 0.7976 - val_loss: 114.3525 - val_ler: 0.8028\n",
      "\n",
      "Epoch 00045: ler did not improve from 0.79688\n",
      "Epoch 46/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 116.4135 - ler: 0.7966 - val_loss: 113.8359 - val_ler: 0.7985\n",
      "\n",
      "Epoch 00046: ler improved from 0.79688 to 0.79658, saving model to ../models/RNN.h5\n",
      "Epoch 47/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 114.6235 - ler: 0.7959 - val_loss: 114.3435 - val_ler: 0.7797\n",
      "\n",
      "Epoch 00047: ler improved from 0.79658 to 0.79588, saving model to ../models/RNN.h5\n",
      "Epoch 48/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 113.4004 - ler: 0.7861 - val_loss: 113.6175 - val_ler: 0.7907\n",
      "\n",
      "Epoch 00048: ler improved from 0.79588 to 0.78606, saving model to ../models/RNN.h5\n",
      "Epoch 49/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.5861 - ler: 0.7924 - val_loss: 114.1342 - val_ler: 0.8033\n",
      "\n",
      "Epoch 00049: ler did not improve from 0.78606\n",
      "Epoch 50/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 114.1370 - ler: 0.7919 - val_loss: 113.8746 - val_ler: 0.7930\n",
      "\n",
      "Epoch 00050: ler did not improve from 0.78606\n",
      "Epoch 51/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 113.7601 - ler: 0.7900 - val_loss: 112.7454 - val_ler: 0.7854\n",
      "\n",
      "Epoch 00051: ler did not improve from 0.78606\n",
      "Epoch 52/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.9254 - ler: 0.7845 - val_loss: 112.5637 - val_ler: 0.7797\n",
      "\n",
      "Epoch 00052: ler improved from 0.78606 to 0.78448, saving model to ../models/RNN.h5\n",
      "Epoch 53/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.3964 - ler: 0.7823 - val_loss: 112.4924 - val_ler: 0.7847\n",
      "\n",
      "Epoch 00053: ler improved from 0.78448 to 0.78229, saving model to ../models/RNN.h5\n",
      "Epoch 54/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 111.0965 - ler: 0.7832 - val_loss: 111.5711 - val_ler: 0.7759\n",
      "\n",
      "Epoch 00054: ler did not improve from 0.78229\n",
      "Epoch 55/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.7366 - ler: 0.7806 - val_loss: 112.1059 - val_ler: 0.7861\n",
      "\n",
      "Epoch 00055: ler improved from 0.78229 to 0.78061, saving model to ../models/RNN.h5\n",
      "Epoch 56/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 109.1547 - ler: 0.7797 - val_loss: 110.8833 - val_ler: 0.7804\n",
      "\n",
      "Epoch 00056: ler improved from 0.78061 to 0.77970, saving model to ../models/RNN.h5\n",
      "Epoch 57/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 111.0593 - ler: 0.7757 - val_loss: 110.7553 - val_ler: 0.7911\n",
      "\n",
      "Epoch 00057: ler improved from 0.77970 to 0.77571, saving model to ../models/RNN.h5\n",
      "Epoch 58/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 109.8839 - ler: 0.7780 - val_loss: 110.2358 - val_ler: 0.7716\n",
      "\n",
      "Epoch 00058: ler did not improve from 0.77571\n",
      "Epoch 59/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.7257 - ler: 0.7724 - val_loss: 111.6223 - val_ler: 0.7794\n",
      "\n",
      "Epoch 00059: ler improved from 0.77571 to 0.77241, saving model to ../models/RNN.h5\n",
      "Epoch 60/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.2878 - ler: 0.7765 - val_loss: 110.9519 - val_ler: 0.7513\n",
      "\n",
      "Epoch 00060: ler did not improve from 0.77241\n",
      "Epoch 61/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 111.3337 - ler: 0.7718 - val_loss: 109.8965 - val_ler: 0.7730\n",
      "\n",
      "Epoch 00061: ler improved from 0.77241 to 0.77185, saving model to ../models/RNN.h5\n",
      "Epoch 62/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 109.8991 - ler: 0.7700 - val_loss: 110.5748 - val_ler: 0.7712\n",
      "\n",
      "Epoch 00062: ler improved from 0.77185 to 0.76999, saving model to ../models/RNN.h5\n",
      "Epoch 63/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 111.2722 - ler: 0.7775 - val_loss: 111.1178 - val_ler: 0.7914\n",
      "\n",
      "Epoch 00063: ler did not improve from 0.76999\n",
      "Epoch 64/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 110.0098 - ler: 0.7752 - val_loss: 108.9516 - val_ler: 0.7673\n",
      "\n",
      "Epoch 00064: ler did not improve from 0.76999\n",
      "Epoch 65/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 107.1592 - ler: 0.7670 - val_loss: 109.6983 - val_ler: 0.7728\n",
      "\n",
      "Epoch 00065: ler improved from 0.76999 to 0.76697, saving model to ../models/RNN.h5\n",
      "Epoch 66/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 109.8401 - ler: 0.7682 - val_loss: 108.6034 - val_ler: 0.7732\n",
      "\n",
      "Epoch 00066: ler did not improve from 0.76697\n",
      "Epoch 67/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 106.0588 - ler: 0.7685 - val_loss: 107.5066 - val_ler: 0.7636\n",
      "\n",
      "Epoch 00067: ler did not improve from 0.76697\n",
      "Epoch 68/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 107.8007 - ler: 0.7652 - val_loss: 107.2398 - val_ler: 0.7517\n",
      "\n",
      "Epoch 00068: ler improved from 0.76697 to 0.76520, saving model to ../models/RNN.h5\n",
      "Epoch 69/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 107.1723 - ler: 0.7597 - val_loss: 107.4709 - val_ler: 0.7603\n",
      "\n",
      "Epoch 00069: ler improved from 0.76520 to 0.75972, saving model to ../models/RNN.h5\n",
      "Epoch 70/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 104.9421 - ler: 0.7608 - val_loss: 106.8421 - val_ler: 0.7670\n",
      "\n",
      "Epoch 00070: ler did not improve from 0.75972\n",
      "Epoch 71/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 106.4477 - ler: 0.7589 - val_loss: 106.1129 - val_ler: 0.7486\n",
      "\n",
      "Epoch 00071: ler improved from 0.75972 to 0.75892, saving model to ../models/RNN.h5\n",
      "Epoch 72/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 108.1586 - ler: 0.7545 - val_loss: 106.5386 - val_ler: 0.7534\n",
      "\n",
      "Epoch 00072: ler improved from 0.75892 to 0.75446, saving model to ../models/RNN.h5\n",
      "Epoch 73/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 106.7403 - ler: 0.7541 - val_loss: 105.7082 - val_ler: 0.7441\n",
      "\n",
      "Epoch 00073: ler improved from 0.75446 to 0.75413, saving model to ../models/RNN.h5\n",
      "Epoch 74/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 105.7487 - ler: 0.7520 - val_loss: 106.4852 - val_ler: 0.7451\n",
      "\n",
      "Epoch 00074: ler improved from 0.75413 to 0.75201, saving model to ../models/RNN.h5\n",
      "Epoch 75/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 105.6998 - ler: 0.7503 - val_loss: 104.6916 - val_ler: 0.7458\n",
      "\n",
      "Epoch 00075: ler improved from 0.75201 to 0.75029, saving model to ../models/RNN.h5\n",
      "Epoch 76/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 104.0136 - ler: 0.7519 - val_loss: 104.8672 - val_ler: 0.7638\n",
      "\n",
      "Epoch 00076: ler did not improve from 0.75029\n",
      "Epoch 77/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 104.8839 - ler: 0.7536 - val_loss: 104.7560 - val_ler: 0.7578\n",
      "\n",
      "Epoch 00077: ler did not improve from 0.75029\n",
      "Epoch 78/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 106.2610 - ler: 0.7505 - val_loss: 103.5074 - val_ler: 0.7319\n",
      "\n",
      "Epoch 00078: ler did not improve from 0.75029\n",
      "Epoch 79/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 104.8003 - ler: 0.7437 - val_loss: 102.9779 - val_ler: 0.7509\n",
      "\n",
      "Epoch 00079: ler improved from 0.75029 to 0.74369, saving model to ../models/RNN.h5\n",
      "Epoch 80/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 101.9478 - ler: 0.7385 - val_loss: 102.2153 - val_ler: 0.7440\n",
      "\n",
      "Epoch 00080: ler improved from 0.74369 to 0.73849, saving model to ../models/RNN.h5\n",
      "Epoch 81/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 101.5389 - ler: 0.7411 - val_loss: 102.1392 - val_ler: 0.7261\n",
      "\n",
      "Epoch 00081: ler did not improve from 0.73849\n",
      "Epoch 82/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 102.2471 - ler: 0.7373 - val_loss: 101.7258 - val_ler: 0.7513\n",
      "\n",
      "Epoch 00082: ler improved from 0.73849 to 0.73732, saving model to ../models/RNN.h5\n",
      "Epoch 83/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 100.9442 - ler: 0.7357 - val_loss: 102.6356 - val_ler: 0.7443\n",
      "\n",
      "Epoch 00083: ler improved from 0.73732 to 0.73573, saving model to ../models/RNN.h5\n",
      "Epoch 84/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 103.5767 - ler: 0.7389 - val_loss: 101.5259 - val_ler: 0.7204\n",
      "\n",
      "Epoch 00084: ler did not improve from 0.73573\n",
      "Epoch 85/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 99.9137 - ler: 0.7309 - val_loss: 100.5041 - val_ler: 0.7235\n",
      "\n",
      "Epoch 00085: ler improved from 0.73573 to 0.73087, saving model to ../models/RNN.h5\n",
      "Epoch 86/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 102.7800 - ler: 0.7266 - val_loss: 99.9594 - val_ler: 0.7226\n",
      "\n",
      "Epoch 00086: ler improved from 0.73087 to 0.72657, saving model to ../models/RNN.h5\n",
      "Epoch 87/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 99.8721 - ler: 0.7270 - val_loss: 101.1035 - val_ler: 0.7288\n",
      "\n",
      "Epoch 00087: ler did not improve from 0.72657\n",
      "Epoch 88/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 102.2787 - ler: 0.7257 - val_loss: 101.1354 - val_ler: 0.7155\n",
      "\n",
      "Epoch 00088: ler improved from 0.72657 to 0.72566, saving model to ../models/RNN.h5\n",
      "Epoch 89/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 98.1778 - ler: 0.7264 - val_loss: 100.6971 - val_ler: 0.7166\n",
      "\n",
      "Epoch 00089: ler did not improve from 0.72566\n",
      "Epoch 90/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 100.2205 - ler: 0.7259 - val_loss: 98.4107 - val_ler: 0.7223\n",
      "\n",
      "Epoch 00090: ler did not improve from 0.72566\n",
      "Epoch 91/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 99.5110 - ler: 0.7205 - val_loss: 98.5912 - val_ler: 0.7288\n",
      "\n",
      "Epoch 00091: ler improved from 0.72566 to 0.72046, saving model to ../models/RNN.h5\n",
      "Epoch 92/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 98.6599 - ler: 0.7201 - val_loss: 98.3297 - val_ler: 0.7304\n",
      "\n",
      "Epoch 00092: ler improved from 0.72046 to 0.72008, saving model to ../models/RNN.h5\n",
      "Epoch 93/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 101.4456 - ler: 0.7145 - val_loss: 97.3143 - val_ler: 0.7049\n",
      "\n",
      "Epoch 00093: ler improved from 0.72008 to 0.71446, saving model to ../models/RNN.h5\n",
      "Epoch 94/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 95.3786 - ler: 0.7134 - val_loss: 96.4172 - val_ler: 0.7254\n",
      "\n",
      "Epoch 00094: ler improved from 0.71446 to 0.71341, saving model to ../models/RNN.h5\n",
      "Epoch 95/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 94.1552 - ler: 0.7104 - val_loss: 100.4115 - val_ler: 0.7179\n",
      "\n",
      "Epoch 00095: ler improved from 0.71341 to 0.71038, saving model to ../models/RNN.h5\n",
      "Epoch 96/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 99.3151 - ler: 0.7164 - val_loss: 98.9079 - val_ler: 0.7190\n",
      "\n",
      "Epoch 00096: ler did not improve from 0.71038\n",
      "Epoch 97/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 100.0248 - ler: 0.7154 - val_loss: 97.3052 - val_ler: 0.7147\n",
      "\n",
      "Epoch 00097: ler did not improve from 0.71038\n",
      "Epoch 98/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 97.6464 - ler: 0.7082 - val_loss: 96.1822 - val_ler: 0.7081\n",
      "\n",
      "Epoch 00098: ler improved from 0.71038 to 0.70820, saving model to ../models/RNN.h5\n",
      "Epoch 99/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 96.3611 - ler: 0.7040 - val_loss: 97.8935 - val_ler: 0.6973\n",
      "\n",
      "Epoch 00099: ler improved from 0.70820 to 0.70399, saving model to ../models/RNN.h5\n",
      "Epoch 100/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 103.9153 - ler: 0.7587 - val_loss: 116.5660 - val_ler: 0.8230\n",
      "\n",
      "Epoch 00100: ler did not improve from 0.70399\n",
      "Epoch 101/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 109.8868 - ler: 0.7850 - val_loss: 106.1008 - val_ler: 0.7491\n",
      "\n",
      "Epoch 00101: ler did not improve from 0.70399\n",
      "Epoch 102/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 103.3371 - ler: 0.7438 - val_loss: 101.1689 - val_ler: 0.7423\n",
      "\n",
      "Epoch 00102: ler did not improve from 0.70399\n",
      "Epoch 103/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 101.7231 - ler: 0.7263 - val_loss: 98.7994 - val_ler: 0.7314\n",
      "\n",
      "Epoch 00103: ler did not improve from 0.70399\n",
      "Epoch 104/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 98.8456 - ler: 0.7261 - val_loss: 97.7610 - val_ler: 0.7135\n",
      "\n",
      "Epoch 00104: ler did not improve from 0.70399\n",
      "Epoch 105/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 97.5336 - ler: 0.7180 - val_loss: 100.2077 - val_ler: 0.7288\n",
      "\n",
      "Epoch 00105: ler did not improve from 0.70399\n",
      "Epoch 106/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 102.8544 - ler: 0.7417 - val_loss: 100.4987 - val_ler: 0.7355\n",
      "\n",
      "Epoch 00106: ler did not improve from 0.70399\n",
      "Epoch 107/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 98.0755 - ler: 0.7276 - val_loss: 97.6456 - val_ler: 0.7188\n",
      "\n",
      "Epoch 00107: ler did not improve from 0.70399\n",
      "Epoch 108/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 95.5505 - ler: 0.7192 - val_loss: 95.8955 - val_ler: 0.7155\n",
      "\n",
      "Epoch 00108: ler did not improve from 0.70399\n",
      "Epoch 109/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 94.0257 - ler: 0.7099 - val_loss: 95.3345 - val_ler: 0.7065\n",
      "\n",
      "Epoch 00109: ler did not improve from 0.70399\n",
      "Epoch 110/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 98.0140 - ler: 0.7171 - val_loss: 99.2741 - val_ler: 0.7088\n",
      "\n",
      "Epoch 00110: ler did not improve from 0.70399\n",
      "Epoch 111/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 97.2708 - ler: 0.7089 - val_loss: 95.9693 - val_ler: 0.6938\n",
      "\n",
      "Epoch 00111: ler did not improve from 0.70399\n",
      "Epoch 112/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 97.5486 - ler: 0.7049 - val_loss: 97.4320 - val_ler: 0.7053\n",
      "\n",
      "Epoch 00112: ler did not improve from 0.70399\n",
      "Epoch 113/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 97.1400 - ler: 0.7067 - val_loss: 94.6504 - val_ler: 0.6932\n",
      "\n",
      "Epoch 00113: ler did not improve from 0.70399\n",
      "Epoch 114/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 94.1674 - ler: 0.7036 - val_loss: 93.5146 - val_ler: 0.7037\n",
      "\n",
      "Epoch 00114: ler improved from 0.70399 to 0.70357, saving model to ../models/RNN.h5\n",
      "Epoch 115/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 91.8863 - ler: 0.6973 - val_loss: 92.6984 - val_ler: 0.7105\n",
      "\n",
      "Epoch 00115: ler improved from 0.70357 to 0.69731, saving model to ../models/RNN.h5\n",
      "Epoch 116/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 93.0779 - ler: 0.6917 - val_loss: 91.8523 - val_ler: 0.7009\n",
      "\n",
      "Epoch 00116: ler improved from 0.69731 to 0.69171, saving model to ../models/RNN.h5\n",
      "Epoch 117/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 92.2381 - ler: 0.6939 - val_loss: 92.3698 - val_ler: 0.6905\n",
      "\n",
      "Epoch 00117: ler did not improve from 0.69171\n",
      "Epoch 118/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 91.8456 - ler: 0.6974 - val_loss: 91.5196 - val_ler: 0.6875\n",
      "\n",
      "Epoch 00118: ler did not improve from 0.69171\n",
      "Epoch 119/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 91.9724 - ler: 0.6920 - val_loss: 91.7186 - val_ler: 0.6896\n",
      "\n",
      "Epoch 00119: ler did not improve from 0.69171\n",
      "Epoch 120/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 92.3990 - ler: 0.6896 - val_loss: 90.4103 - val_ler: 0.6899\n",
      "\n",
      "Epoch 00120: ler improved from 0.69171 to 0.68964, saving model to ../models/RNN.h5\n",
      "Epoch 121/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 90.5343 - ler: 0.6823 - val_loss: 89.2260 - val_ler: 0.6834\n",
      "\n",
      "Epoch 00121: ler improved from 0.68964 to 0.68226, saving model to ../models/RNN.h5\n",
      "Epoch 122/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 87.1990 - ler: 0.6811 - val_loss: 88.7395 - val_ler: 0.6747\n",
      "\n",
      "Epoch 00122: ler improved from 0.68226 to 0.68108, saving model to ../models/RNN.h5\n",
      "Epoch 123/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 85.8211 - ler: 0.6756 - val_loss: 88.2132 - val_ler: 0.6837\n",
      "\n",
      "Epoch 00123: ler improved from 0.68108 to 0.67563, saving model to ../models/RNN.h5\n",
      "Epoch 124/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 90.2582 - ler: 0.6747 - val_loss: 87.4320 - val_ler: 0.6720\n",
      "\n",
      "Epoch 00124: ler improved from 0.67563 to 0.67475, saving model to ../models/RNN.h5\n",
      "Epoch 125/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 88.5160 - ler: 0.6667 - val_loss: 87.4970 - val_ler: 0.6779\n",
      "\n",
      "Epoch 00125: ler improved from 0.67475 to 0.66671, saving model to ../models/RNN.h5\n",
      "Epoch 126/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 89.0553 - ler: 0.6684 - val_loss: 86.6568 - val_ler: 0.6664\n",
      "\n",
      "Epoch 00126: ler did not improve from 0.66671\n",
      "Epoch 127/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 88.7542 - ler: 0.6669 - val_loss: 85.9794 - val_ler: 0.6516\n",
      "\n",
      "Epoch 00127: ler did not improve from 0.66671\n",
      "Epoch 128/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 87.1059 - ler: 0.6601 - val_loss: 85.3831 - val_ler: 0.6535\n",
      "\n",
      "Epoch 00128: ler improved from 0.66671 to 0.66012, saving model to ../models/RNN.h5\n",
      "Epoch 129/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 86.3272 - ler: 0.6647 - val_loss: 89.2582 - val_ler: 0.6682\n",
      "\n",
      "Epoch 00129: ler did not improve from 0.66012\n",
      "Epoch 130/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 87.8910 - ler: 0.6615 - val_loss: 85.2230 - val_ler: 0.6489\n",
      "\n",
      "Epoch 00130: ler did not improve from 0.66012\n",
      "Epoch 131/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 86.1422 - ler: 0.6711 - val_loss: 96.6766 - val_ler: 0.7065\n",
      "\n",
      "Epoch 00131: ler did not improve from 0.66012\n",
      "Epoch 132/300\n",
      "46/46 [==============================] - 3s 55ms/step - loss: 93.9006 - ler: 0.6844 - val_loss: 89.3532 - val_ler: 0.6808\n",
      "\n",
      "Epoch 00132: ler did not improve from 0.66012\n",
      "Epoch 133/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 90.0024 - ler: 0.6688 - val_loss: 86.5990 - val_ler: 0.6629\n",
      "\n",
      "Epoch 00133: ler did not improve from 0.66012\n",
      "Epoch 134/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 88.7616 - ler: 0.6640 - val_loss: 85.7602 - val_ler: 0.6554\n",
      "\n",
      "Epoch 00134: ler did not improve from 0.66012\n",
      "Epoch 135/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 84.8576 - ler: 0.6550 - val_loss: 85.3493 - val_ler: 0.6488\n",
      "\n",
      "Epoch 00135: ler improved from 0.66012 to 0.65499, saving model to ../models/RNN.h5\n",
      "Epoch 136/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 84.3951 - ler: 0.6557 - val_loss: 83.8489 - val_ler: 0.6446\n",
      "\n",
      "Epoch 00136: ler did not improve from 0.65499\n",
      "Epoch 137/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 84.2933 - ler: 0.6483 - val_loss: 83.1991 - val_ler: 0.6470\n",
      "\n",
      "Epoch 00137: ler improved from 0.65499 to 0.64826, saving model to ../models/RNN.h5\n",
      "Epoch 138/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 85.4107 - ler: 0.6426 - val_loss: 82.2030 - val_ler: 0.6377\n",
      "\n",
      "Epoch 00138: ler improved from 0.64826 to 0.64264, saving model to ../models/RNN.h5\n",
      "Epoch 139/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 83.1543 - ler: 0.6466 - val_loss: 86.5772 - val_ler: 0.6504\n",
      "\n",
      "Epoch 00139: ler did not improve from 0.64264\n",
      "Epoch 140/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 87.8930 - ler: 0.6667 - val_loss: 85.2426 - val_ler: 0.6492\n",
      "\n",
      "Epoch 00140: ler did not improve from 0.64264\n",
      "Epoch 141/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 84.0047 - ler: 0.6498 - val_loss: 83.2572 - val_ler: 0.6389\n",
      "\n",
      "Epoch 00141: ler did not improve from 0.64264\n",
      "Epoch 142/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 81.1190 - ler: 0.6473 - val_loss: 83.1017 - val_ler: 0.6513\n",
      "\n",
      "Epoch 00142: ler did not improve from 0.64264\n",
      "Epoch 143/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 84.4454 - ler: 0.6395 - val_loss: 81.2996 - val_ler: 0.6281\n",
      "\n",
      "Epoch 00143: ler improved from 0.64264 to 0.63947, saving model to ../models/RNN.h5\n",
      "Epoch 144/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 82.0427 - ler: 0.6350 - val_loss: 80.0657 - val_ler: 0.6328\n",
      "\n",
      "Epoch 00144: ler improved from 0.63947 to 0.63502, saving model to ../models/RNN.h5\n",
      "Epoch 145/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 80.9215 - ler: 0.6279 - val_loss: 88.3650 - val_ler: 0.6312\n",
      "\n",
      "Epoch 00145: ler improved from 0.63502 to 0.62785, saving model to ../models/RNN.h5\n",
      "Epoch 146/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 87.5144 - ler: 0.6437 - val_loss: 81.7748 - val_ler: 0.6225\n",
      "\n",
      "Epoch 00146: ler did not improve from 0.62785\n",
      "Epoch 147/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 78.0558 - ler: 0.6293 - val_loss: 81.4741 - val_ler: 0.6290\n",
      "\n",
      "Epoch 00147: ler did not improve from 0.62785\n",
      "Epoch 148/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 82.1497 - ler: 0.6310 - val_loss: 79.0995 - val_ler: 0.6164\n",
      "\n",
      "Epoch 00148: ler did not improve from 0.62785\n",
      "Epoch 149/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 78.6594 - ler: 0.6218 - val_loss: 77.9738 - val_ler: 0.6137\n",
      "\n",
      "Epoch 00149: ler improved from 0.62785 to 0.62184, saving model to ../models/RNN.h5\n",
      "Epoch 150/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 79.2078 - ler: 0.6186 - val_loss: 77.4568 - val_ler: 0.6042\n",
      "\n",
      "Epoch 00150: ler improved from 0.62184 to 0.61855, saving model to ../models/RNN.h5\n",
      "Epoch 151/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 79.0305 - ler: 0.6139 - val_loss: 76.0958 - val_ler: 0.6092\n",
      "\n",
      "Epoch 00151: ler improved from 0.61855 to 0.61386, saving model to ../models/RNN.h5\n",
      "Epoch 152/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 79.1017 - ler: 0.6068 - val_loss: 75.2461 - val_ler: 0.6003\n",
      "\n",
      "Epoch 00152: ler improved from 0.61386 to 0.60681, saving model to ../models/RNN.h5\n",
      "Epoch 153/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 76.6873 - ler: 0.6037 - val_loss: 81.9991 - val_ler: 0.6277\n",
      "\n",
      "Epoch 00153: ler improved from 0.60681 to 0.60367, saving model to ../models/RNN.h5\n",
      "Epoch 154/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 81.9386 - ler: 0.6275 - val_loss: 78.4967 - val_ler: 0.6146\n",
      "\n",
      "Epoch 00154: ler did not improve from 0.60367\n",
      "Epoch 155/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 77.5341 - ler: 0.6126 - val_loss: 75.9044 - val_ler: 0.6042\n",
      "\n",
      "Epoch 00155: ler did not improve from 0.60367\n",
      "Epoch 156/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 76.5935 - ler: 0.6034 - val_loss: 75.1522 - val_ler: 0.5901\n",
      "\n",
      "Epoch 00156: ler improved from 0.60367 to 0.60340, saving model to ../models/RNN.h5\n",
      "Epoch 157/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 75.0047 - ler: 0.5979 - val_loss: 73.8093 - val_ler: 0.5792\n",
      "\n",
      "Epoch 00157: ler improved from 0.60340 to 0.59788, saving model to ../models/RNN.h5\n",
      "Epoch 158/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 73.7850 - ler: 0.5913 - val_loss: 73.6689 - val_ler: 0.5833\n",
      "\n",
      "Epoch 00158: ler improved from 0.59788 to 0.59132, saving model to ../models/RNN.h5\n",
      "Epoch 159/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.9176 - ler: 0.5923 - val_loss: 72.5646 - val_ler: 0.5838\n",
      "\n",
      "Epoch 00159: ler did not improve from 0.59132\n",
      "Epoch 160/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 76.7646 - ler: 0.6015 - val_loss: 89.1332 - val_ler: 0.6587\n",
      "\n",
      "Epoch 00160: ler did not improve from 0.59132\n",
      "Epoch 161/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 85.8627 - ler: 0.6284 - val_loss: 80.3673 - val_ler: 0.6154\n",
      "\n",
      "Epoch 00161: ler did not improve from 0.59132\n",
      "Epoch 162/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 81.0883 - ler: 0.6109 - val_loss: 77.8265 - val_ler: 0.6089\n",
      "\n",
      "Epoch 00162: ler did not improve from 0.59132\n",
      "Epoch 163/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 78.6451 - ler: 0.6015 - val_loss: 75.1530 - val_ler: 0.5920\n",
      "\n",
      "Epoch 00163: ler did not improve from 0.59132\n",
      "Epoch 164/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 74.3368 - ler: 0.5946 - val_loss: 75.8955 - val_ler: 0.5843\n",
      "\n",
      "Epoch 00164: ler did not improve from 0.59132\n",
      "Epoch 165/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 77.9767 - ler: 0.6013 - val_loss: 73.5115 - val_ler: 0.5831\n",
      "\n",
      "Epoch 00165: ler did not improve from 0.59132\n",
      "Epoch 166/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.1064 - ler: 0.5804 - val_loss: 74.4404 - val_ler: 0.5932\n",
      "\n",
      "Epoch 00166: ler improved from 0.59132 to 0.58037, saving model to ../models/RNN.h5\n",
      "Epoch 167/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 77.4671 - ler: 0.5991 - val_loss: 75.0834 - val_ler: 0.5776\n",
      "\n",
      "Epoch 00167: ler did not improve from 0.58037\n",
      "Epoch 168/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 74.9590 - ler: 0.5846 - val_loss: 71.9859 - val_ler: 0.5791\n",
      "\n",
      "Epoch 00168: ler did not improve from 0.58037\n",
      "Epoch 169/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.6600 - ler: 0.5804 - val_loss: 72.0320 - val_ler: 0.5708\n",
      "\n",
      "Epoch 00169: ler did not improve from 0.58037\n",
      "Epoch 170/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.9238 - ler: 0.5765 - val_loss: 70.7707 - val_ler: 0.5657\n",
      "\n",
      "Epoch 00170: ler improved from 0.58037 to 0.57648, saving model to ../models/RNN.h5\n",
      "Epoch 171/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.4787 - ler: 0.5633 - val_loss: 69.9855 - val_ler: 0.5562\n",
      "\n",
      "Epoch 00171: ler improved from 0.57648 to 0.56326, saving model to ../models/RNN.h5\n",
      "Epoch 172/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.0500 - ler: 0.5631 - val_loss: 68.6229 - val_ler: 0.5470\n",
      "\n",
      "Epoch 00172: ler improved from 0.56326 to 0.56311, saving model to ../models/RNN.h5\n",
      "Epoch 173/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.5516 - ler: 0.5590 - val_loss: 67.7869 - val_ler: 0.5582\n",
      "\n",
      "Epoch 00173: ler improved from 0.56311 to 0.55899, saving model to ../models/RNN.h5\n",
      "Epoch 174/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.9965 - ler: 0.5605 - val_loss: 71.7810 - val_ler: 0.5568\n",
      "\n",
      "Epoch 00174: ler did not improve from 0.55899\n",
      "Epoch 175/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 70.1597 - ler: 0.5692 - val_loss: 92.8781 - val_ler: 0.6547\n",
      "\n",
      "Epoch 00175: ler did not improve from 0.55899\n",
      "Epoch 176/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 85.2291 - ler: 0.6176 - val_loss: 78.3393 - val_ler: 0.5923\n",
      "\n",
      "Epoch 00176: ler did not improve from 0.55899\n",
      "Epoch 177/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 78.4249 - ler: 0.5948 - val_loss: 71.7764 - val_ler: 0.5721\n",
      "\n",
      "Epoch 00177: ler did not improve from 0.55899\n",
      "Epoch 178/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.9391 - ler: 0.5739 - val_loss: 69.7544 - val_ler: 0.5576\n",
      "\n",
      "Epoch 00178: ler did not improve from 0.55899\n",
      "Epoch 179/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 70.3019 - ler: 0.5613 - val_loss: 68.9075 - val_ler: 0.5589\n",
      "\n",
      "Epoch 00179: ler did not improve from 0.55899\n",
      "Epoch 180/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 69.1864 - ler: 0.5729 - val_loss: 69.6429 - val_ler: 0.5509\n",
      "\n",
      "Epoch 00180: ler did not improve from 0.55899\n",
      "Epoch 181/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 69.9989 - ler: 0.5579 - val_loss: 67.2198 - val_ler: 0.5389\n",
      "\n",
      "Epoch 00181: ler improved from 0.55899 to 0.55786, saving model to ../models/RNN.h5\n",
      "Epoch 182/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.3755 - ler: 0.5484 - val_loss: 66.7339 - val_ler: 0.5479\n",
      "\n",
      "Epoch 00182: ler improved from 0.55786 to 0.54837, saving model to ../models/RNN.h5\n",
      "Epoch 183/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.6173 - ler: 0.5488 - val_loss: 68.1676 - val_ler: 0.5448\n",
      "\n",
      "Epoch 00183: ler did not improve from 0.54837\n",
      "Epoch 184/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 82.7973 - ler: 0.6848 - val_loss: 117.8954 - val_ler: 0.7795\n",
      "\n",
      "Epoch 00184: ler did not improve from 0.54837\n",
      "Epoch 185/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 112.6733 - ler: 0.7442 - val_loss: 101.3889 - val_ler: 0.7354\n",
      "\n",
      "Epoch 00185: ler did not improve from 0.54837\n",
      "Epoch 186/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 95.5815 - ler: 0.7026 - val_loss: 94.4488 - val_ler: 0.7031\n",
      "\n",
      "Epoch 00186: ler did not improve from 0.54837\n",
      "Epoch 187/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 92.9003 - ler: 0.6763 - val_loss: 87.1381 - val_ler: 0.6708\n",
      "\n",
      "Epoch 00187: ler did not improve from 0.54837\n",
      "Epoch 188/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 86.7551 - ler: 0.6584 - val_loss: 83.5330 - val_ler: 0.6434\n",
      "\n",
      "Epoch 00188: ler did not improve from 0.54837\n",
      "Epoch 189/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 81.4631 - ler: 0.6410 - val_loss: 81.4587 - val_ler: 0.6369\n",
      "\n",
      "Epoch 00189: ler did not improve from 0.54837\n",
      "Epoch 190/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 81.2314 - ler: 0.6438 - val_loss: 80.3609 - val_ler: 0.6341\n",
      "\n",
      "Epoch 00190: ler did not improve from 0.54837\n",
      "Epoch 191/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 79.1673 - ler: 0.6215 - val_loss: 77.2237 - val_ler: 0.6129\n",
      "\n",
      "Epoch 00191: ler did not improve from 0.54837\n",
      "Epoch 192/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 77.4140 - ler: 0.6099 - val_loss: 75.5785 - val_ler: 0.6026\n",
      "\n",
      "Epoch 00192: ler did not improve from 0.54837\n",
      "Epoch 193/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 75.8337 - ler: 0.6060 - val_loss: 74.4026 - val_ler: 0.5936\n",
      "\n",
      "Epoch 00193: ler did not improve from 0.54837\n",
      "Epoch 194/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 77.2187 - ler: 0.6042 - val_loss: 74.4306 - val_ler: 0.6000\n",
      "\n",
      "Epoch 00194: ler did not improve from 0.54837\n",
      "Epoch 195/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 75.2670 - ler: 0.5907 - val_loss: 72.4960 - val_ler: 0.5747\n",
      "\n",
      "Epoch 00195: ler did not improve from 0.54837\n",
      "Epoch 196/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.5883 - ler: 0.5858 - val_loss: 71.7208 - val_ler: 0.5795\n",
      "\n",
      "Epoch 00196: ler did not improve from 0.54837\n",
      "Epoch 197/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.5056 - ler: 0.5789 - val_loss: 69.7101 - val_ler: 0.5612\n",
      "\n",
      "Epoch 00197: ler did not improve from 0.54837\n",
      "Epoch 198/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 69.0000 - ler: 0.5715 - val_loss: 68.9972 - val_ler: 0.5553\n",
      "\n",
      "Epoch 00198: ler did not improve from 0.54837\n",
      "Epoch 199/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.3409 - ler: 0.5678 - val_loss: 69.9111 - val_ler: 0.5665\n",
      "\n",
      "Epoch 00199: ler did not improve from 0.54837\n",
      "Epoch 200/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 76.4881 - ler: 0.5982 - val_loss: 71.9225 - val_ler: 0.5710\n",
      "\n",
      "Epoch 00200: ler did not improve from 0.54837\n",
      "Epoch 201/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 71.7260 - ler: 0.5753 - val_loss: 68.4353 - val_ler: 0.5566\n",
      "\n",
      "Epoch 00201: ler did not improve from 0.54837\n",
      "Epoch 202/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 70.3853 - ler: 0.5605 - val_loss: 67.9451 - val_ler: 0.5394\n",
      "\n",
      "Epoch 00202: ler did not improve from 0.54837\n",
      "Epoch 203/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 70.6737 - ler: 0.5527 - val_loss: 66.1018 - val_ler: 0.5370\n",
      "\n",
      "Epoch 00203: ler did not improve from 0.54837\n",
      "Epoch 204/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 66.8594 - ler: 0.5727 - val_loss: 78.6967 - val_ler: 0.5994\n",
      "\n",
      "Epoch 00204: ler did not improve from 0.54837\n",
      "Epoch 205/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 72.7218 - ler: 0.5894 - val_loss: 71.0662 - val_ler: 0.5776\n",
      "\n",
      "Epoch 00205: ler did not improve from 0.54837\n",
      "Epoch 206/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 70.5972 - ler: 0.5611 - val_loss: 66.3320 - val_ler: 0.5436\n",
      "\n",
      "Epoch 00206: ler did not improve from 0.54837\n",
      "Epoch 207/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 65.9339 - ler: 0.5511 - val_loss: 67.2377 - val_ler: 0.5385\n",
      "\n",
      "Epoch 00207: ler did not improve from 0.54837\n",
      "Epoch 208/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.0610 - ler: 0.5492 - val_loss: 66.4535 - val_ler: 0.5316\n",
      "\n",
      "Epoch 00208: ler did not improve from 0.54837\n",
      "Epoch 209/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 66.9765 - ler: 0.5401 - val_loss: 64.9155 - val_ler: 0.5305\n",
      "\n",
      "Epoch 00209: ler improved from 0.54837 to 0.54006, saving model to ../models/RNN.h5\n",
      "Epoch 210/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 65.0747 - ler: 0.5364 - val_loss: 63.4101 - val_ler: 0.5141\n",
      "\n",
      "Epoch 00210: ler improved from 0.54006 to 0.53641, saving model to ../models/RNN.h5\n",
      "Epoch 211/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 65.2027 - ler: 0.5309 - val_loss: 63.4914 - val_ler: 0.5256\n",
      "\n",
      "Epoch 00211: ler improved from 0.53641 to 0.53091, saving model to ../models/RNN.h5\n",
      "Epoch 212/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 66.1411 - ler: 0.5291 - val_loss: 62.8867 - val_ler: 0.5165\n",
      "\n",
      "Epoch 00212: ler improved from 0.53091 to 0.52913, saving model to ../models/RNN.h5\n",
      "Epoch 213/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 64.1878 - ler: 0.5237 - val_loss: 62.1024 - val_ler: 0.5245\n",
      "\n",
      "Epoch 00213: ler improved from 0.52913 to 0.52374, saving model to ../models/RNN.h5\n",
      "Epoch 214/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 61.7592 - ler: 0.5157 - val_loss: 60.1506 - val_ler: 0.4931\n",
      "\n",
      "Epoch 00214: ler improved from 0.52374 to 0.51566, saving model to ../models/RNN.h5\n",
      "Epoch 215/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 58.0226 - ler: 0.5051 - val_loss: 59.6390 - val_ler: 0.4937\n",
      "\n",
      "Epoch 00215: ler improved from 0.51566 to 0.50511, saving model to ../models/RNN.h5\n",
      "Epoch 216/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 60.3943 - ler: 0.5029 - val_loss: 59.8966 - val_ler: 0.4927\n",
      "\n",
      "Epoch 00216: ler improved from 0.50511 to 0.50294, saving model to ../models/RNN.h5\n",
      "Epoch 217/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.3596 - ler: 0.4949 - val_loss: 58.4202 - val_ler: 0.4848\n",
      "\n",
      "Epoch 00217: ler improved from 0.50294 to 0.49491, saving model to ../models/RNN.h5\n",
      "Epoch 218/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.3242 - ler: 0.4976 - val_loss: 58.6509 - val_ler: 0.4855\n",
      "\n",
      "Epoch 00218: ler did not improve from 0.49491\n",
      "Epoch 219/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 61.1964 - ler: 0.4918 - val_loss: 58.2919 - val_ler: 0.4791\n",
      "\n",
      "Epoch 00219: ler improved from 0.49491 to 0.49180, saving model to ../models/RNN.h5\n",
      "Epoch 220/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.4609 - ler: 0.4881 - val_loss: 58.2061 - val_ler: 0.4839\n",
      "\n",
      "Epoch 00220: ler improved from 0.49180 to 0.48809, saving model to ../models/RNN.h5\n",
      "Epoch 221/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.0848 - ler: 0.4841 - val_loss: 56.4417 - val_ler: 0.4724\n",
      "\n",
      "Epoch 00221: ler improved from 0.48809 to 0.48414, saving model to ../models/RNN.h5\n",
      "Epoch 222/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 61.2150 - ler: 0.5145 - val_loss: 70.0638 - val_ler: 0.5654\n",
      "\n",
      "Epoch 00222: ler did not improve from 0.48414\n",
      "Epoch 223/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 68.5856 - ler: 0.5336 - val_loss: 61.5832 - val_ler: 0.4990\n",
      "\n",
      "Epoch 00223: ler did not improve from 0.48414\n",
      "Epoch 224/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 63.1949 - ler: 0.5019 - val_loss: 59.1816 - val_ler: 0.4774\n",
      "\n",
      "Epoch 00224: ler did not improve from 0.48414\n",
      "Epoch 225/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 57.8860 - ler: 0.4897 - val_loss: 57.2246 - val_ler: 0.4662\n",
      "\n",
      "Epoch 00225: ler did not improve from 0.48414\n",
      "Epoch 226/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 57.3131 - ler: 0.4765 - val_loss: 57.8767 - val_ler: 0.4830\n",
      "\n",
      "Epoch 00226: ler improved from 0.48414 to 0.47650, saving model to ../models/RNN.h5\n",
      "Epoch 227/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 57.4107 - ler: 0.4854 - val_loss: 55.6925 - val_ler: 0.4720\n",
      "\n",
      "Epoch 00227: ler did not improve from 0.47650\n",
      "Epoch 228/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.4589 - ler: 0.4736 - val_loss: 54.9724 - val_ler: 0.4553\n",
      "\n",
      "Epoch 00228: ler improved from 0.47650 to 0.47363, saving model to ../models/RNN.h5\n",
      "Epoch 229/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 55.0512 - ler: 0.4637 - val_loss: 61.8608 - val_ler: 0.5032\n",
      "\n",
      "Epoch 00229: ler improved from 0.47363 to 0.46371, saving model to ../models/RNN.h5\n",
      "Epoch 230/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 64.2952 - ler: 0.5042 - val_loss: 58.3978 - val_ler: 0.4734\n",
      "\n",
      "Epoch 00230: ler did not improve from 0.46371\n",
      "Epoch 231/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 60.5517 - ler: 0.4814 - val_loss: 55.8160 - val_ler: 0.4500\n",
      "\n",
      "Epoch 00231: ler did not improve from 0.46371\n",
      "Epoch 232/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.5328 - ler: 0.4762 - val_loss: 59.0205 - val_ler: 0.4792\n",
      "\n",
      "Epoch 00232: ler did not improve from 0.46371\n",
      "Epoch 233/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.3969 - ler: 0.4801 - val_loss: 55.4226 - val_ler: 0.4614\n",
      "\n",
      "Epoch 00233: ler did not improve from 0.46371\n",
      "Epoch 234/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 55.6073 - ler: 0.4639 - val_loss: 54.1589 - val_ler: 0.4493\n",
      "\n",
      "Epoch 00234: ler did not improve from 0.46371\n",
      "Epoch 235/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 53.7219 - ler: 0.4526 - val_loss: 53.2874 - val_ler: 0.4365\n",
      "\n",
      "Epoch 00235: ler improved from 0.46371 to 0.45260, saving model to ../models/RNN.h5\n",
      "Epoch 236/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 53.8894 - ler: 0.4504 - val_loss: 54.5724 - val_ler: 0.4472\n",
      "\n",
      "Epoch 00236: ler improved from 0.45260 to 0.45035, saving model to ../models/RNN.h5\n",
      "Epoch 237/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.4734 - ler: 0.4695 - val_loss: 54.3541 - val_ler: 0.4460\n",
      "\n",
      "Epoch 00237: ler did not improve from 0.45035\n",
      "Epoch 238/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.6275 - ler: 0.4603 - val_loss: 51.8335 - val_ler: 0.4262\n",
      "\n",
      "Epoch 00238: ler did not improve from 0.45035\n",
      "Epoch 239/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 52.9378 - ler: 0.4427 - val_loss: 50.8239 - val_ler: 0.4157\n",
      "\n",
      "Epoch 00239: ler improved from 0.45035 to 0.44270, saving model to ../models/RNN.h5\n",
      "Epoch 240/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 52.0721 - ler: 0.4391 - val_loss: 50.6169 - val_ler: 0.4304\n",
      "\n",
      "Epoch 00240: ler improved from 0.44270 to 0.43910, saving model to ../models/RNN.h5\n",
      "Epoch 241/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 50.5775 - ler: 0.4305 - val_loss: 50.0322 - val_ler: 0.4129\n",
      "\n",
      "Epoch 00241: ler improved from 0.43910 to 0.43053, saving model to ../models/RNN.h5\n",
      "Epoch 242/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 50.4698 - ler: 0.4269 - val_loss: 49.1494 - val_ler: 0.4126\n",
      "\n",
      "Epoch 00242: ler improved from 0.43053 to 0.42695, saving model to ../models/RNN.h5\n",
      "Epoch 243/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 50.1182 - ler: 0.4233 - val_loss: 49.7139 - val_ler: 0.4148\n",
      "\n",
      "Epoch 00243: ler improved from 0.42695 to 0.42327, saving model to ../models/RNN.h5\n",
      "Epoch 244/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 51.4650 - ler: 0.4268 - val_loss: 48.7996 - val_ler: 0.4109\n",
      "\n",
      "Epoch 00244: ler did not improve from 0.42327\n",
      "Epoch 245/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 50.1415 - ler: 0.4205 - val_loss: 48.6595 - val_ler: 0.3976\n",
      "\n",
      "Epoch 00245: ler improved from 0.42327 to 0.42050, saving model to ../models/RNN.h5\n",
      "Epoch 246/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 48.8563 - ler: 0.4132 - val_loss: 48.0824 - val_ler: 0.4007\n",
      "\n",
      "Epoch 00246: ler improved from 0.42050 to 0.41323, saving model to ../models/RNN.h5\n",
      "Epoch 247/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 49.6086 - ler: 0.4119 - val_loss: 47.0954 - val_ler: 0.3861\n",
      "\n",
      "Epoch 00247: ler improved from 0.41323 to 0.41192, saving model to ../models/RNN.h5\n",
      "Epoch 248/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 45.9482 - ler: 0.3983 - val_loss: 46.5605 - val_ler: 0.3812\n",
      "\n",
      "Epoch 00248: ler improved from 0.41192 to 0.39827, saving model to ../models/RNN.h5\n",
      "Epoch 249/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 47.4477 - ler: 0.4032 - val_loss: 46.3676 - val_ler: 0.3763\n",
      "\n",
      "Epoch 00249: ler did not improve from 0.39827\n",
      "Epoch 250/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 48.3090 - ler: 0.4015 - val_loss: 50.7979 - val_ler: 0.4116\n",
      "\n",
      "Epoch 00250: ler did not improve from 0.39827\n",
      "Epoch 251/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 76.9529 - ler: 0.5693 - val_loss: 67.3290 - val_ler: 0.5277\n",
      "\n",
      "Epoch 00251: ler did not improve from 0.39827\n",
      "Epoch 252/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 66.0479 - ler: 0.5077 - val_loss: 57.2928 - val_ler: 0.4709\n",
      "\n",
      "Epoch 00252: ler did not improve from 0.39827\n",
      "Epoch 253/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.3965 - ler: 0.4794 - val_loss: 57.9513 - val_ler: 0.4689\n",
      "\n",
      "Epoch 00253: ler did not improve from 0.39827\n",
      "Epoch 254/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 58.2994 - ler: 0.4803 - val_loss: 57.2027 - val_ler: 0.4608\n",
      "\n",
      "Epoch 00254: ler did not improve from 0.39827\n",
      "Epoch 255/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.4372 - ler: 0.4848 - val_loss: 82.8313 - val_ler: 0.5540\n",
      "\n",
      "Epoch 00255: ler did not improve from 0.39827\n",
      "Epoch 256/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 79.1195 - ler: 0.5633 - val_loss: 69.9694 - val_ler: 0.5295\n",
      "\n",
      "Epoch 00256: ler did not improve from 0.39827\n",
      "Epoch 257/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 69.0906 - ler: 0.5292 - val_loss: 64.2354 - val_ler: 0.5138\n",
      "\n",
      "Epoch 00257: ler did not improve from 0.39827\n",
      "Epoch 258/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 62.2464 - ler: 0.4999 - val_loss: 58.2354 - val_ler: 0.4880\n",
      "\n",
      "Epoch 00258: ler did not improve from 0.39827\n",
      "Epoch 259/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 59.1834 - ler: 0.4752 - val_loss: 55.6113 - val_ler: 0.4519\n",
      "\n",
      "Epoch 00259: ler did not improve from 0.39827\n",
      "Epoch 260/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 56.7450 - ler: 0.4609 - val_loss: 52.3306 - val_ler: 0.4416\n",
      "\n",
      "Epoch 00260: ler did not improve from 0.39827\n",
      "Epoch 261/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 53.6381 - ler: 0.4492 - val_loss: 52.9544 - val_ler: 0.4454\n",
      "\n",
      "Epoch 00261: ler did not improve from 0.39827\n",
      "Epoch 262/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 54.5415 - ler: 0.4436 - val_loss: 49.9198 - val_ler: 0.4232\n",
      "\n",
      "Epoch 00262: ler did not improve from 0.39827\n",
      "Epoch 263/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 52.6326 - ler: 0.4318 - val_loss: 48.9969 - val_ler: 0.4072\n",
      "\n",
      "Epoch 00263: ler did not improve from 0.39827\n",
      "Epoch 264/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 49.3807 - ler: 0.4204 - val_loss: 48.0821 - val_ler: 0.4055\n",
      "\n",
      "Epoch 00264: ler did not improve from 0.39827\n",
      "Epoch 265/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 49.4546 - ler: 0.4155 - val_loss: 47.2309 - val_ler: 0.3882\n",
      "\n",
      "Epoch 00265: ler did not improve from 0.39827\n",
      "Epoch 266/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 46.7618 - ler: 0.4065 - val_loss: 46.5364 - val_ler: 0.3941\n",
      "\n",
      "Epoch 00266: ler did not improve from 0.39827\n",
      "Epoch 267/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 47.3859 - ler: 0.3966 - val_loss: 45.4802 - val_ler: 0.3740\n",
      "\n",
      "Epoch 00267: ler improved from 0.39827 to 0.39655, saving model to ../models/RNN.h5\n",
      "Epoch 268/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 46.3314 - ler: 0.3977 - val_loss: 45.5486 - val_ler: 0.3857\n",
      "\n",
      "Epoch 00268: ler did not improve from 0.39655\n",
      "Epoch 269/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 47.5977 - ler: 0.3943 - val_loss: 45.1631 - val_ler: 0.3747\n",
      "\n",
      "Epoch 00269: ler improved from 0.39655 to 0.39426, saving model to ../models/RNN.h5\n",
      "Epoch 270/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 45.4052 - ler: 0.3888 - val_loss: 43.9829 - val_ler: 0.3698\n",
      "\n",
      "Epoch 00270: ler improved from 0.39426 to 0.38881, saving model to ../models/RNN.h5\n",
      "Epoch 271/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 46.0097 - ler: 0.3853 - val_loss: 43.4097 - val_ler: 0.3624\n",
      "\n",
      "Epoch 00271: ler improved from 0.38881 to 0.38526, saving model to ../models/RNN.h5\n",
      "Epoch 272/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 45.4446 - ler: 0.3839 - val_loss: 43.8308 - val_ler: 0.3672\n",
      "\n",
      "Epoch 00272: ler improved from 0.38526 to 0.38390, saving model to ../models/RNN.h5\n",
      "Epoch 273/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 45.2914 - ler: 0.3756 - val_loss: 42.8973 - val_ler: 0.3568\n",
      "\n",
      "Epoch 00273: ler improved from 0.38390 to 0.37561, saving model to ../models/RNN.h5\n",
      "Epoch 274/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 43.8070 - ler: 0.3721 - val_loss: 43.1146 - val_ler: 0.3488\n",
      "\n",
      "Epoch 00274: ler improved from 0.37561 to 0.37214, saving model to ../models/RNN.h5\n",
      "Epoch 275/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 43.5641 - ler: 0.3713 - val_loss: 44.1197 - val_ler: 0.3681\n",
      "\n",
      "Epoch 00275: ler improved from 0.37214 to 0.37126, saving model to ../models/RNN.h5\n",
      "Epoch 276/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 44.3461 - ler: 0.3700 - val_loss: 42.3438 - val_ler: 0.3468\n",
      "\n",
      "Epoch 00276: ler improved from 0.37126 to 0.37002, saving model to ../models/RNN.h5\n",
      "Epoch 277/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 43.3305 - ler: 0.3618 - val_loss: 41.4218 - val_ler: 0.3367\n",
      "\n",
      "Epoch 00277: ler improved from 0.37002 to 0.36182, saving model to ../models/RNN.h5\n",
      "Epoch 278/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 42.4360 - ler: 0.3557 - val_loss: 40.9430 - val_ler: 0.3413\n",
      "\n",
      "Epoch 00278: ler improved from 0.36182 to 0.35573, saving model to ../models/RNN.h5\n",
      "Epoch 279/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 43.7093 - ler: 0.3658 - val_loss: 40.9844 - val_ler: 0.3430\n",
      "\n",
      "Epoch 00279: ler did not improve from 0.35573\n",
      "Epoch 280/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 42.0923 - ler: 0.3547 - val_loss: 40.0239 - val_ler: 0.3251\n",
      "\n",
      "Epoch 00280: ler improved from 0.35573 to 0.35466, saving model to ../models/RNN.h5\n",
      "Epoch 281/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 41.6415 - ler: 0.3482 - val_loss: 39.9252 - val_ler: 0.3278\n",
      "\n",
      "Epoch 00281: ler improved from 0.35466 to 0.34818, saving model to ../models/RNN.h5\n",
      "Epoch 282/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 41.2375 - ler: 0.3450 - val_loss: 39.6440 - val_ler: 0.3308\n",
      "\n",
      "Epoch 00282: ler improved from 0.34818 to 0.34504, saving model to ../models/RNN.h5\n",
      "Epoch 283/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 42.2851 - ler: 0.3464 - val_loss: 39.3428 - val_ler: 0.3201\n",
      "\n",
      "Epoch 00283: ler did not improve from 0.34504\n",
      "Epoch 284/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 40.5888 - ler: 0.3405 - val_loss: 39.0802 - val_ler: 0.3214\n",
      "\n",
      "Epoch 00284: ler improved from 0.34504 to 0.34045, saving model to ../models/RNN.h5\n",
      "Epoch 285/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 39.9649 - ler: 0.3432 - val_loss: 38.6882 - val_ler: 0.3233\n",
      "\n",
      "Epoch 00285: ler did not improve from 0.34045\n",
      "Epoch 286/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 40.7007 - ler: 0.3371 - val_loss: 38.2272 - val_ler: 0.3168\n",
      "\n",
      "Epoch 00286: ler improved from 0.34045 to 0.33714, saving model to ../models/RNN.h5\n",
      "Epoch 287/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 39.8826 - ler: 0.3305 - val_loss: 38.1051 - val_ler: 0.3101\n",
      "\n",
      "Epoch 00287: ler improved from 0.33714 to 0.33046, saving model to ../models/RNN.h5\n",
      "Epoch 288/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 38.1721 - ler: 0.3254 - val_loss: 37.9056 - val_ler: 0.3182\n",
      "\n",
      "Epoch 00288: ler improved from 0.33046 to 0.32543, saving model to ../models/RNN.h5\n",
      "Epoch 289/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 39.3271 - ler: 0.3339 - val_loss: 37.7681 - val_ler: 0.3167\n",
      "\n",
      "Epoch 00289: ler did not improve from 0.32543\n",
      "Epoch 290/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 38.8721 - ler: 0.3285 - val_loss: 38.0963 - val_ler: 0.3035\n",
      "\n",
      "Epoch 00290: ler did not improve from 0.32543\n",
      "Epoch 291/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 39.1642 - ler: 0.3329 - val_loss: 37.3554 - val_ler: 0.2958\n",
      "\n",
      "Epoch 00291: ler did not improve from 0.32543\n",
      "Epoch 292/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 37.6435 - ler: 0.3198 - val_loss: 36.8215 - val_ler: 0.3019\n",
      "\n",
      "Epoch 00292: ler improved from 0.32543 to 0.31976, saving model to ../models/RNN.h5\n",
      "Epoch 293/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 38.6015 - ler: 0.3181 - val_loss: 36.2825 - val_ler: 0.2952\n",
      "\n",
      "Epoch 00293: ler improved from 0.31976 to 0.31808, saving model to ../models/RNN.h5\n",
      "Epoch 294/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 37.4616 - ler: 0.3180 - val_loss: 36.3598 - val_ler: 0.2983\n",
      "\n",
      "Epoch 00294: ler improved from 0.31808 to 0.31798, saving model to ../models/RNN.h5\n",
      "Epoch 295/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 37.2033 - ler: 0.3194 - val_loss: 36.4884 - val_ler: 0.3041\n",
      "\n",
      "Epoch 00295: ler did not improve from 0.31798\n",
      "Epoch 296/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 37.2786 - ler: 0.3139 - val_loss: 35.3921 - val_ler: 0.2833\n",
      "\n",
      "Epoch 00296: ler improved from 0.31798 to 0.31387, saving model to ../models/RNN.h5\n",
      "Epoch 297/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 37.2828 - ler: 0.3078 - val_loss: 35.5181 - val_ler: 0.2925\n",
      "\n",
      "Epoch 00297: ler improved from 0.31387 to 0.30780, saving model to ../models/RNN.h5\n",
      "Epoch 298/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 36.5305 - ler: 0.3039 - val_loss: 34.8745 - val_ler: 0.2734\n",
      "\n",
      "Epoch 00298: ler improved from 0.30780 to 0.30394, saving model to ../models/RNN.h5\n",
      "Epoch 299/300\n",
      "46/46 [==============================] - 2s 54ms/step - loss: 38.0624 - ler: 0.3057 - val_loss: 35.4640 - val_ler: 0.2849\n",
      "\n",
      "Epoch 00299: ler did not improve from 0.30394\n",
      "Epoch 300/300\n",
      "46/46 [==============================] - 2s 53ms/step - loss: 36.5935 - ler: 0.3051 - val_loss: 34.4493 - val_ler: 0.2792\n",
      "\n",
      "Epoch 00300: ler did not improve from 0.30394\n"
     ]
    }
   ],
   "source": [
    "# Compile Training Model with selected optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(initial_learning_rate, momentum)\n",
    "model_train.compile(optimizer=optimizer)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../models/'+\"RNN\"+'.h5',monitor='ler',verbose=1, save_best_only=True, mode='min')\n",
    "# ModelCheckpoint(filepath='../models/'+\"RNN\"+'.h5', verbose=0,)\n",
    "\n",
    "# Training, Our y is already defined so no need# mlflow.tensorflow.autolog()\n",
    "history = model_train.fit(x=[train_inputs, train_targets, train_seq_len, train_targets_len], y=None,\n",
    "                validation_data=([val_inputs, val_targets, val_seq_len, val_targets_len], None),\n",
    "                batch_size=batch_size, epochs=300,callbacks=[checkpointer])\n",
    "\n",
    "# try:\n",
    "#     experiment_id = mlflow.create_experiment(\"Stacked RNN(LSTM): 50 Cells\")\n",
    "#     experiment = mlflow.get_experiment(experiment_id)\n",
    "# except mlflow.exceptions.MlflowException:\n",
    "#     experiment = mlflow.get_experiment_by_name(\"Stacked RNN(LSTM): 50 Cells\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3297fc44-8655-439d-8442-87b8905793fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'ler', 'val_loss', 'val_ler'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
    "plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d2819bd-fbb4-43ee-8f1e-5bf82b0f89c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "     \n",
      "    \n",
      "        \n",
      "        \n",
      "Decoded:\n",
      "     \n",
      "    \n",
      "        \n",
      "       \n",
      "                \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "# Decoding\n",
    "print('Original:')\n",
    "print(original_list[0])\n",
    "print(original_list[1])\n",
    "print(original_list[2])\n",
    "print(original_list[3])\n",
    "print('Decoded:')\n",
    "\n",
    "\n",
    "train_inputs = tf.ragged.constant([i for i in inputs_list[:6]], dtype=np.float32)\n",
    "train_seq_len = tf.cast(train_inputs.row_lengths(), tf.int32)\n",
    "train_inputs = train_inputs.to_tensor(default_value=FEAT_MASK_VALUE)\n",
    "\n",
    "decoded, _ = tf.nn.ctc_greedy_decoder(tf.transpose(\n",
    "    model_predict.predict(train_inputs), (1, 0, 2)), train_seq_len)\n",
    "\n",
    "d = tf.sparse.to_dense(decoded[0], default_value=-1).numpy()\n",
    "str_decoded = [''.join([alphabets['num_to_char'][str(x)]\n",
    "                       for x in np.asarray(row) if x != -1]) for row in d]\n",
    "\n",
    "# print('decoded',str_decoded)\n",
    "for s in str_decoded:\n",
    "    # Replacing blank label to none\n",
    "    # s = s.replace(chr(ord('z') + 1), '')\n",
    "    # Replacing space label to space\n",
    "    s = s.replace(alphabets['num_to_char']['0'], ' ')\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d422e-0f52-4970-a7f3-d5a2aabe639a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
